<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian Processes · GpABC</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://tanhevg.github.io/GpABC.jl/stable/ref-gp/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">GpABC</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Package Overview</span><ul><li><a class="tocitem" href="../overview-abc/">ABC Parameter Inference</a></li><li><a class="tocitem" href="../overview-ms/">ABC Model Selection</a></li><li><a class="tocitem" href="../overview-lna/">LNA</a></li><li><a class="tocitem" href="../overview-gp/">Gaussian Process Regression</a></li><li><a class="tocitem" href="../summary_stats/">Summary Statistics</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../example-abc/">ABC Parameter Inference</a></li><li><a class="tocitem" href="../example-ms/">ABC Model Selection</a></li><li><a class="tocitem" href="../example-lna/">Stochastic inference (LNA)</a></li><li><a class="tocitem" href="../example-gp/">Gaussian Processes</a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../ref-abc/">ABC Basic</a></li><li><a class="tocitem" href="../ref-abc-advanced/">ABC Advanced</a></li><li><a class="tocitem" href="../ref-lna/">Stochastic inference (LNA)</a></li><li><a class="tocitem" href="../ref-ms/">Model Selection</a></li><li class="is-active"><a class="tocitem" href>Gaussian Processes</a><ul class="internal"><li><a class="tocitem" href="#Index"><span>Index</span></a></li><li><a class="tocitem" href="#Types-and-Functions"><span>Types and Functions</span></a></li></ul></li><li><a class="tocitem" href="../ref-kernels/">Kernels</a></li></ul></li><li><a class="tocitem" href="../faq/">FAQ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Gaussian Processes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gaussian Processes</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/tanhevg/GpABC.jl/blob/master/docs/src/ref-gp.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Gaussian-Processes-Reference"><a class="docs-heading-anchor" href="#Gaussian-Processes-Reference">Gaussian Processes Reference</a><a id="Gaussian-Processes-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-Processes-Reference" title="Permalink"></a></h1><p><code>GpABC</code> functions for Gaussian Process Regression. See also <a href="../overview-gp/#gp-overview">Gaussian Processes Overview</a>, <a href="../example-gp/#Gaussian-Processes-Examples">Gaussian Processes Examples</a>.</p><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel-Tuple{}"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.gp_loglikelihood-Tuple{GPModel}"><code>GpABC.gp_loglikelihood</code></a></li><li><a href="#GpABC.gp_loglikelihood-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood</code></a></li><li><a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood_grad</code></a></li><li><a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood_log</code></a></li><li><a href="#GpABC.gp_regression-Tuple{GPModel}"><code>GpABC.gp_regression</code></a></li><li><a href="#GpABC.gp_regression-Tuple{AbstractVecOrMat{Float64}, GPModel}"><code>GpABC.gp_regression</code></a></li><li><a href="#GpABC.gp_regression_sample"><code>GpABC.gp_regression_sample</code></a></li><li><a href="#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>GpABC.gp_train</code></a></li><li><a href="#GpABC.set_hyperparameters-Tuple{GPModel, AbstractVector{Float64}}"><code>GpABC.set_hyperparameters</code></a></li></ul><h2 id="Types-and-Functions"><a class="docs-heading-anchor" href="#Types-and-Functions">Types and Functions</a><a id="Types-and-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Types-and-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        kernel::AbstractGPKernel
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a> that allows the kernel to be specified. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L65-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Default constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a>, that will use <a href="../ref-kernels/#GpABC.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a>. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L51-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GPModel</code></pre><p>The main type that is used by most functions within the package.</p><p>All data matrices are row-major.</p><p><strong>Fields</strong></p><ul><li><code>kernel::AbstractGPKernel</code>: the kernel</li><li><code>gp_training_x::AbstractArray{Float64, 2}</code>: training <code>x</code>. Size: <span>$n \times d$</span>.</li><li><code>gp_training_y::AbstractArray{Float64, 2}</code>: training <code>y</code>. Size: <span>$n \times 1$</span>.</li><li><code>gp_test_x::AbstractArray{Float64, 2}</code>: test <code>x</code>.  Size: <span>$m \times d$</span>.</li><li><code>gp_hyperparameters::AbstractArray{Float64, 1}</code>: kernel hyperparameters, followed by standard deviation of intrinsic noise <span>$\sigma_n$</span>, which is always the last element in the array.</li><li><code>cache::HPOptimisationCache</code>: cache of matrices that can be re-used between calls to <code>gp_loglikelihood</code> and <code>gp_loglikelihood_grad</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L22-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.GPModel-Tuple{}" href="#GpABC.GPModel-Tuple{}"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">GPModel(;training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    kernel::AbstractGPKernel=SquaredExponentialIsoKernel(),
    gp_hyperparameters::AbstractArray{Float64, 1}=Array{Float64}(0))</code></pre><p>Constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a> with explicit arguments. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L81-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_loglikelihood-Tuple{AbstractVector{Float64}, GPModel}" href="#GpABC.gp_loglikelihood-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_loglikelihood(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L176-L178">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_loglikelihood-Tuple{GPModel}" href="#GpABC.gp_loglikelihood-Tuple{GPModel}"><code>GpABC.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_loglikelihood(gpm::GPModel)</code></pre><p>Compute the log likelihood function, based on the kernel and training data specified in <code>gpm</code>.</p><p class="math-container">\[log p(y \vert X, \theta) = - \frac{1}{2}(y^TK^{-1}y + log \vert K \vert + n log 2 \pi)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L166-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_loglikelihood_grad-Tuple{AbstractVector{Float64}, GPModel}" href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood_grad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_loglikelihood_grad(theta::AbstractArray{Float64, 1}, gpem::GPModel)</code></pre><p>Gradient of the log likelihood function (<a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractVector{Float64}, GPModel}"><code>gp_loglikelihood_log</code></a>) with respect to logged hyperparameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L181-L186">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_loglikelihood_log-Tuple{AbstractVector{Float64}, GPModel}" href="#GpABC.gp_loglikelihood_log-Tuple{AbstractVector{Float64}, GPModel}"><code>GpABC.gp_loglikelihood_log</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_loglikelihood_log(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre><p>Log likelihood function with log hyperparameters. This is the target function of the hyperparameters optimisation procedure. Its gradient is computed by <a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractVector{Float64}, GPModel}"><code>gp_loglikelihood_grad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L149-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_regression-Tuple{AbstractVecOrMat{Float64}, GPModel}" href="#GpABC.gp_regression-Tuple{AbstractVecOrMat{Float64}, GPModel}"><code>GpABC.gp_regression</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_regression(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}},
    gpem::GPModel; &lt;optional keyword arguments&gt;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L231-L234">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_regression-Tuple{GPModel}" href="#GpABC.gp_regression-Tuple{GPModel}"><code>GpABC.gp_regression</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_regression(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Run the Gaussian Process Regression.</p><p><strong>Arguments</strong></p><ul><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</li><li><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size <span>$m \times d$</span>.</li><li><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_regression</code> print basic information to standard output.</li><li><code>full_covariance_matrix::Bool</code> (optional): whether we need the full covariance matrix, or just the variance vector. Defaults to <code>false</code> (i.e. just the variance).</li><li><code>batch_size::Int</code> (optional): If <code>full_covariance_matrix</code> is set to <code>false</code>, then the mean and variance vectors will be computed in batches of this size, to avoid allocating huge matrices. Defaults to 1000.</li><li><code>observation_noise::Bool</code> (optional): whether the observation noise (with variance <span>$\sigma_n^2$</span>) should be included in the output variance. Defaults to <code>true</code>.</li></ul><p><strong>Return</strong></p><p>A tuple of <code>(mean, var)</code>. <code>mean</code> is a mean vector of the output multivariate Normal distribution, of size <span>$m$</span>. <code>var</code> is either the covariance matrix of size <span>$m \times m$</span>, or a variance vector of size <span>$m$</span>, depending on <code>full_covariance_matrix</code> flag.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L203-L226">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_regression_sample" href="#GpABC.gp_regression_sample"><code>GpABC.gp_regression_sample</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gp_regression_sample(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}}, n_samples::Int64, gpem::GPModel)</code></pre><p>Return <code>n_samples</code> random samples from the Gaussian process posterior, evaluated at <code>test_x</code>. <a href="#GpABC.gp_regression-Tuple{AbstractVecOrMat{Float64}, GPModel}"><code>gp_regression</code></a>.</p><p><strong>Arguments</strong></p><ul><li><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size <span>$m \times d$</span>.</li><li><code>n_samples</code>: integer specifying the number of posterior samples.</li><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</li><li><code>full_cov_matrix</code>: whether to use the full covariance matrix or just its diagonal elements (default <code>true</code>).</li></ul><p><strong>Return</strong></p><p>An array of posterior samples with shape <span>$m \times$</span> <code>n_samples</code> if <code>n_samples</code>&gt;1 and <span>$m$</span> otherwise.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L337-L350">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.set_hyperparameters-Tuple{GPModel, AbstractVector{Float64}}" href="#GpABC.set_hyperparameters-Tuple{GPModel, AbstractVector{Float64}}"><code>GpABC.set_hyperparameters</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">set_hyperparameters(gpm::GPModel, hypers::AbstractArray{Float64, 1})</code></pre><p>Set the hyperparameters of the <a href="#GpABC.GPModel"><code>GPModel</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp.jl#L134-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer" href="#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>GpABC.gp_train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gp_train(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Find Maximum Likelihood Estimate of Gaussian Process hyperparameters by maximising <a href="#GpABC.gp_loglikelihood-Tuple{AbstractVector{Float64}, GPModel}"><code>gp_loglikelihood</code></a>, using <a href="http://julianlsolvers.github.io/Optim.jl/stable/"><code>Optim</code></a> package. The optimisation target is <a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractVector{Float64}, GPModel}"><code>gp_loglikelihood_log</code></a>, with gradient computed by <a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractVector{Float64}, GPModel}"><code>gp_loglikelihood_grad</code></a>. Internally, this function optimises the MLE with respect to logarithms of hyperparameters. This is done for numerical stability. Logarithmisation and exponentiation is performed by this function, i.e. real hyperparameters, not logarithms, are taken in and returned back.</p><p>By default, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/cg/">Conjugate Gradient</a> bounded box optimisation is used, as long as the gradient with respect to hyperparameters (<a href="../ref-kernels/#GpABC.covariance_grad-Tuple{AbstractGPKernel, AbstractVector{Float64}, AbstractMatrix{Float64}, AbstractMatrix{Float64}}"><code>covariance_grad</code></a>) is implemented for the kernel function. If the gradient implementation is not provided, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/">Nelder Mead</a> optimiser is used by default.</p><p><strong>Mandatory argument</strong></p><ul><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel and the starting hyperparameters that will be used for optimisation.</li></ul><p><strong>Optional keyword arguments</strong></p><ul><li><code>optimiser::Type{&lt;:Optim.AbstractOptimizer}</code>: the solver to use. If not given, then <code>ConjugateGradient</code> will be used for kernels that have gradient implementation, and <code>NelderMead</code> will be used for those that don&#39;t.</li><li><code>hp_lower::AbstractArray{Float64, 1}</code>: the lower boundary for box optimisation. Defaults to <span>$e^{-10}$</span> for all hyperparameters.</li><li><code>hp_upper::AbstractArray{Float64, 1}</code>: the upper boundary for box optimisation. Defaults to <span>$e^{10}$</span> for all hyperparameters.</li><li><code>log_level::Int</code>: log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_train</code> print basic information to standard output. <code>2</code> switches <code>Optim</code> logging on, in addition to <code>1</code>.</li></ul><p><strong>Return</strong></p><p>The list of all hyperparameters, including the standard deviation of the measurement noise <span>$\sigma_n$</span>. Note that after this function returns, the hyperparameters of <code>gpm</code> will be set to the optimised value, and there is no need to call <a href="#GpABC.set_hyperparameters-Tuple{GPModel, AbstractVector{Float64}}"><code>set_hyperparameters</code></a> once again.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/1f65a998a3ca8c20b94e4ddce05578f2ea32a3d4/src/gp/gp_optimisation.jl#L17-L49">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ref-ms/">« Model Selection</a><a class="docs-footer-nextpage" href="../ref-kernels/">Kernels »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Tuesday 31 January 2023 16:32">Tuesday 31 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
