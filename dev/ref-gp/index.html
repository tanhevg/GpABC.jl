<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian Processes · GpABC</title><link rel="canonical" href="https://tanhevg.github.io/GpABC.jl/stable/ref-gp/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpABC</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">Package Overview</span><ul><li><a class="toctext" href="../overview-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../overview-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../overview-lna/">LNA</a></li><li><a class="toctext" href="../overview-gp/">Gaussian Process Regression</a></li><li><a class="toctext" href="../summary_stats/">Summary Statistics</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../example-abc/">ABC Parameter Inference</a></li><li><a class="toctext" href="../example-ms/">ABC Model Selection</a></li><li><a class="toctext" href="../example-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../example-gp/">Gaussian Processes</a></li></ul></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../ref-abc/">ABC Basic</a></li><li><a class="toctext" href="../ref-abc-advanced/">ABC Advanced</a></li><li><a class="toctext" href="../ref-lna/">Stochastic inference (LNA)</a></li><li><a class="toctext" href="../ref-ms/">Model Selection</a></li><li class="current"><a class="toctext" href>Gaussian Processes</a><ul class="internal"><li><a class="toctext" href="#Index-1">Index</a></li><li><a class="toctext" href="#Types-and-Functions-1">Types and Functions</a></li></ul></li><li><a class="toctext" href="../ref-kernels/">Kernels</a></li></ul></li><li><a class="toctext" href="../faq/">FAQ</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Gaussian Processes</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpABC.jl/blob/master/docs/src/ref-gp.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Gaussian Processes</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Gaussian-Processes-Reference-1" href="#Gaussian-Processes-Reference-1">Gaussian Processes Reference</a></h1><p><code>GpABC</code> functions for Gaussian Process Regression. See also <a href="../overview-gp/#gp-overview-1">Gaussian Processes Overview</a>, <a href="../example-gp/#Gaussian-Processes-Examples-1">Gaussian Processes Examples</a>.</p><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><li><a href="#GpABC.GPModel-Tuple{}"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.GPModel"><code>GpABC.GPModel</code></a></li><li><a href="#GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood</code></a></li><li><a href="#GpABC.gp_loglikelihood-Tuple{GPModel}"><code>GpABC.gp_loglikelihood</code></a></li><li><a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood_grad</code></a></li><li><a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood_log</code></a></li><li><a href="#GpABC.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GPModel}"><code>GpABC.gp_regression</code></a></li><li><a href="#GpABC.gp_regression-Tuple{GPModel}"><code>GpABC.gp_regression</code></a></li><li><a href="#GpABC.gp_regression_sample"><code>GpABC.gp_regression_sample</code></a></li><li><a href="#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>GpABC.gp_train</code></a></li><li><a href="#GpABC.set_hyperparameters-Tuple{GPModel,AbstractArray{Float64,1}}"><code>GpABC.set_hyperparameters</code></a></li></ul><h2><a class="nav-anchor" id="Types-and-Functions-1" href="#Types-and-Functions-1">Types and Functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">GPModel</code></pre><p>The main type that is used by most functions within the package.</p><p>All data matrices are row-major.</p><p><strong>Fields</strong></p><ul><li><code>kernel::AbstractGPKernel</code>: the kernel</li><li><code>gp_training_x::AbstractArray{Float64, 2}</code>: training <code>x</code>. Size: <span>$n \times d$</span>.</li><li><code>gp_training_y::AbstractArray{Float64, 2}</code>: training <code>y</code>. Size: <span>$n \times 1$</span>.</li><li><code>gp_test_x::AbstractArray{Float64, 2}</code>: test <code>x</code>.  Size: <span>$m \times d$</span>.</li><li><code>gp_hyperparameters::AbstractArray{Float64, 1}</code>: kernel hyperparameters, followed by standard deviation of intrinsic noise <span>$\sigma_n$</span>, which is always the last element in the array.</li><li><code>cache::HPOptimisationCache</code>: cache of matrices that can be re-used between calls to <code>gp_loglikelihood</code> and <code>gp_loglikelihood_grad</code></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L22-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        kernel::AbstractGPKernel
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a> that allows the kernel to be specified. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L65-L73">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.GPModel" href="#GpABC.GPModel"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Default constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a>, that will use <a href="../ref-kernels/#GpABC.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a>. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L51-L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.GPModel-Tuple{}" href="#GpABC.GPModel-Tuple{}"><code>GpABC.GPModel</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">GPModel(;training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    kernel::AbstractGPKernel=SquaredExponentialIsoKernel(),
    gp_hyperparameters::AbstractArray{Float64, 1}=Array{Float64}(0))</code></pre><p>Constructor of <a href="#GpABC.GPModel"><code>GPModel</code></a> with explicit arguments. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L81-L90">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GPModel}" href="#GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_loglikelihood(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L176-L178">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_loglikelihood-Tuple{GPModel}" href="#GpABC.gp_loglikelihood-Tuple{GPModel}"><code>GpABC.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_loglikelihood(gpm::GPModel)</code></pre><p>Compute the log likelihood function, based on the kernel and training data specified in <code>gpm</code>.</p><div>\[log p(y \vert X, \theta) = - \frac{1}{2}(y^TK^{-1}y + log \vert K \vert + n log 2 \pi)\]</div></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L166-L173">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}" href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_loglikelihood_grad(theta::AbstractArray{Float64, 1}, gpem::GPModel)</code></pre><p>Gradient of the log likelihood function (<a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_log</code></a>) with respect to logged hyperparameters.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L181-L186">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}" href="#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}"><code>GpABC.gp_loglikelihood_log</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_loglikelihood_log(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre><p>Log likelihood function with log hyperparameters. This is the target function of the hyperparameters optimisation procedure. Its gradient is coputed by <a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_grad</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L149-L154">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_regression-Tuple{GPModel}" href="#GpABC.gp_regression-Tuple{GPModel}"><code>GpABC.gp_regression</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_regression(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Run the Gaussian Process Regression.</p><p><strong>Arguments</strong></p><ul><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</li><li><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size <span>$m \times d$</span>.</li><li><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_regression</code> print basic information to standard output.</li><li><code>full_covariance_matrix::Bool</code> (optional): whether we need the full covariance matrix, or just the variance vector. Defaults to <code>false</code> (i.e. just the variance).</li><li><code>batch_size::Int</code> (optional): If <code>full_covariance_matrix</code> is set to <code>false</code>, then the mean and variance vectors will be computed in batches of this size, to avoid allocating huge matrices. Defaults to 1000.</li><li><code>observation_noise::Bool</code> (optional): whether the observation noise (with variance <span>$\sigma_n^2$</span>) should be included in the output variance. Defaults to <code>true</code>.</li></ul><p><strong>Return</strong></p><p>A tuple of <code>(mean, var)</code>. <code>mean</code> is a mean vector of the output multivariate Normal distribution, of size <span>$m$</span>. <code>var</code> is either the covariance matrix of size <span>$m \times m$</span>, or a variance vector of size <span>$m$</span>, depending on <code>full_covariance_matrix</code> flag.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L203-L226">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GPModel}" href="#GpABC.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GPModel}"><code>GpABC.gp_regression</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_regression(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}},
    gpem::GPModel; &lt;optional keyword arguments&gt;)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L231-L234">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_regression_sample" href="#GpABC.gp_regression_sample"><code>GpABC.gp_regression_sample</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">gp_regression_sample(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}}, n_samples::Int64, gpem::GPModel)</code></pre><p>Return <code>n_samples</code> random samples from the Gaussian process posterior, evaluated at <code>test_x</code>. <a href="#GpABC.gp_regression-Tuple{GPModel}"><code>gp_regression</code></a>.</p><p><strong>Arguments</strong></p><ul><li><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size <span>$m \times d$</span>.</li><li><code>n_samples</code>: integer specifying the number of posterior samples.</li><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</li><li><code>full_cov_matrix</code>: whether to use the full covariance matrix or just its diagonal elements (default <code>true</code>).</li></ul><p><strong>Return</strong></p><p>An array of posterior samples with shape <span>$m \times$</span> <code>n_samples</code> if <code>n_samples</code>&gt;1 and <span>$m$</span> otherwise.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L337-L350">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.set_hyperparameters-Tuple{GPModel,AbstractArray{Float64,1}}" href="#GpABC.set_hyperparameters-Tuple{GPModel,AbstractArray{Float64,1}}"><code>GpABC.set_hyperparameters</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">set_hyperparameters(gpm::GPModel, hypers::AbstractArray{Float64, 1})</code></pre><p>Set the hyperparameters of the <a href="#GpABC.GPModel"><code>GPModel</code></a></p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp.jl#L134-L138">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer" href="#GpABC.gp_train-Union{Tuple{GPModel}, Tuple{TOpt}} where TOpt&lt;:Optim.AbstractOptimizer"><code>GpABC.gp_train</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">gp_train(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Find Maximum Likelihood Estimate of Gaussian Process hyperparameters by maximising <a href="#GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood</code></a>, using <a href="http://julianlsolvers.github.io/Optim.jl/stable/"><code>Optim</code></a> package. The optimisation target is <a href="#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_log</code></a>, with gradient computed by <a href="#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GPModel}"><code>gp_loglikelihood_grad</code></a>. Internally, this function optimises the MLE with respect to logarithms of hyperparameters. This is done for numerical stability. Logarithmisation and exponentiation is performed by this funtion, i.e. real hyperparameters, not logarithms, are taken in and returned back.</p><p>By default, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/cg/">Conjugate Gradient</a> bounded box optimisation is used, as long as the gradient with respect to hyperparameters (<a href="../ref-kernels/#GpABC.covariance_grad-Tuple{AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a>) is implemented for the kernel function. If the gradient implementation is not provided, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/">Nelder Mead</a> optimiser is used by default.</p><p><strong>Mandatory argument</strong></p><ul><li><code>gpm</code>: the <a href="#GpABC.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel and the starting hyperparameters that will be used for optimisation.</li></ul><p><strong>Optional keyword arguments</strong></p><ul><li><code>optimiser::Type{&lt;:Optim.AbstractOptimizer}</code>: the solver to use. If not given, then <code>ConjugateGradient</code> will be used for kernels that have gradient implementation, and <code>NelderMead</code> will be used for those that don&#39;t.</li><li><code>hp_lower::AbstractArray{Float64, 1}</code>: the lower boundary for box optimisation. Defaults to <span>$e^{-10}$</span> for all hyperparameters.</li><li><code>hp_upper::AbstractArray{Float64, 1}</code>: the upper boundary for box optimisation. Defaults to <span>$e^{10}$</span> for all hyperparameters.</li><li><code>log_level::Int</code>: log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_train</code> print basic information to standard output. <code>2</code> switches <code>Optim</code> logging on, in addition to <code>1</code>.</li></ul><p><strong>Return</strong></p><p>The list of all hyperparameters, including the standard deviation of the measurement noise <span>$\sigma_n$</span>. Note that after this function returns, the hyperparameters of <code>gpm</code> will be set to the optimised value, and there is no need to call <a href="#GpABC.set_hyperparameters-Tuple{GPModel,AbstractArray{Float64,1}}"><code>set_hyperparameters</code></a> once again.</p></div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpABC.jl/blob/38dadef058b44daf2d612666f08bd6433903469e/src/gp/gp_optimisation.jl#L17-L49">source</a></section><footer><hr/><a class="previous" href="../ref-ms/"><span class="direction">Previous</span><span class="title">Model Selection</span></a><a class="next" href="../ref-kernels/"><span class="direction">Next</span><span class="title">Kernels</span></a></footer></article></body></html>
